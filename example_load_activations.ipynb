{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a3b77987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "from zarr import Array, Group\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "store = zarr.storage.ZipStore(\n",
    "    \"activations-gemma2-2b-the-stack-json-100k/activations_part_0000.zarr.zip\",\n",
    "    read_only=True,\n",
    ")\n",
    "z = zarr.open(store, mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "42b951ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "├── <span style=\"font-weight: bold\">activations</span>\n",
       "│   ├── <span style=\"font-weight: bold\">layer_0</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_1</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_10</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_11</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_12</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_13</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_14</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_15</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_16</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_17</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_18</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_19</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_2</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_20</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_21</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_22</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_23</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_24</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_25</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_26</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_3</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_4</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_5</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_6</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_7</span> (80, 256, 2304) float16\n",
       "│   ├── <span style=\"font-weight: bold\">layer_8</span> (80, 256, 2304) float16\n",
       "│   └── <span style=\"font-weight: bold\">layer_9</span> (80, 256, 2304) float16\n",
       "├── <span style=\"font-weight: bold\">attention_mask</span> (80, 256) int32\n",
       "└── <span style=\"font-weight: bold\">input_ids</span> (80, 256) int32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "├── \u001b[1mactivations\u001b[0m\n",
       "│   ├── \u001b[1mlayer_0\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_1\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_10\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_11\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_12\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_13\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_14\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_15\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_16\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_17\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_18\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_19\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_2\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_20\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_21\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_22\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_23\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_24\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_25\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_26\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_3\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_4\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_5\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_6\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_7\u001b[0m (80, 256, 2304) float16\n",
       "│   ├── \u001b[1mlayer_8\u001b[0m (80, 256, 2304) float16\n",
       "│   └── \u001b[1mlayer_9\u001b[0m (80, 256, 2304) float16\n",
       "├── \u001b[1mattention_mask\u001b[0m (80, 256) int32\n",
       "└── \u001b[1minput_ids\u001b[0m (80, 256) int32\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76faa9c5",
   "metadata": {},
   "source": [
    "A single zarr archive is stored stored as a `Group` with three sub-elements -- `activations`, `attention_mask` and `input_ids`. \n",
    "- The group `activations` consist of `n_layers` 3D sub-arrays of shape `(batch_size, sequence_length, hidden_dimension)`.\n",
    "- The data inside the `attention_mask` is particularly important because it determines the actual length of each of the sequences. Since some of them may be shorter than max_sequence_length, the useless pad tokens need to be removed before processing.\n",
    "- `input_ids` contains the actual input ids used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4b226ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_length(z: Group, sample_id: int) -> int:\n",
    "    attention_mask = z[\"attention_mask\"][sample_id]\n",
    "    return attention_mask.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b6ead3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_latent(\n",
    "    z: Group,\n",
    "    sample_id: int,\n",
    "    layer_id: int,\n",
    "    position: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Retrieve the latent activation for a specific batch, sample, layer, and position.\"\"\"  # noqa: E501\n",
    "    assert position < get_seq_length(z, sample_id), (  # noqa: S101\n",
    "        f\"Position out of bounds. Max position for sample {sample_id} is {get_seq_length(z, sample_id) - 1}.\"  # noqa: E501\n",
    "    )\n",
    "\n",
    "    return z[\"activations\"][f\"layer_{layer_id}\"][sample_id, position]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2992f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.006042, -0.2812  ,  1.086   , ...,  0.703   , -0.3828  ,\n",
       "       -0.59    ], dtype=float16)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_one_latent(z, 1, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5dce5317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.006042, -0.2812  ,  1.086   , ...,  0.703   , -0.3828  ,\n",
       "        -0.59    ],\n",
       "       [ 0.006042, -0.2812  ,  1.086   , ...,  0.703   , -0.3828  ,\n",
       "        -0.59    ],\n",
       "       [ 0.006042, -0.2812  ,  1.086   , ...,  0.703   , -0.3828  ,\n",
       "        -0.59    ],\n",
       "       ...,\n",
       "       [ 0.006042, -0.2812  ,  1.086   , ...,  0.703   , -0.3828  ,\n",
       "        -0.59    ],\n",
       "       [ 0.006042, -0.2812  ,  1.086   , ...,  0.703   , -0.3828  ,\n",
       "        -0.59    ],\n",
       "       [ 0.006042, -0.2812  ,  1.086   , ...,  0.703   , -0.3828  ,\n",
       "        -0.59    ]], dtype=float16)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[\"activations\"][\"layer_0\"][\n",
    "    :,\n",
    "    0,\n",
    "]  # Accessing the first layer's activations for all samples at position 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e7fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
