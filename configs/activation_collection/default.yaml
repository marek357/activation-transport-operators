# Global settings
seed: 42
experiment_name: activation_collection_500k_tokens

# Processing configuration
batch_size: 128 # Adjust based on your GPU memory
max_length: 256 # Maximum sequence length
num_tokens: 500_000 # Target number of tokens to collect
save_every_n_batches: 30 # Save intermediate results every N batches

# DataLoader optimization settings
num_workers: 2 # Number of worker processes for data loading
pin_memory: true # Pin memory for faster GPU transfer
prefetch_factor: 2 # Number of batches to prefetch per worker

# Storage configuration
output_dir: "activations" # Directory to save activation files
cache_dir: "./cache" # Directory to cache tokenized datasets
storage_dtype: "float16" # Data type for storing activations (saves memory)

# Memory management
enable_memory_logging: true
enable_garbage_collection: true
